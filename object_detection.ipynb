{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f34dd35",
   "metadata": {},
   "source": [
    "Download IP Webcam from playstore for mobile streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "536d991f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you are not having the ip address, you can type: 0 \n",
      "Enter the ip address of the phone: 0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "# Opencv DNN\n",
    "net = cv2.dnn.readNet(\"yolov4-tiny.weights\", \"yolov4-tiny.cfg\")\n",
    "model = cv2.dnn_DetectionModel(net)\n",
    "model.setInputParams(size=(416, 416), scale=1/255)\n",
    "\n",
    "# Load class lists\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as file_object:\n",
    "    for class_name in file_object.readlines():\n",
    "        class_name = class_name.strip()\n",
    "        classes.append(class_name)\n",
    "\n",
    "\n",
    "# Initialize camera\n",
    "print(\"If you are not having the ip address, you can type: 0 \")\n",
    "ip = input(\"Enter the ip address of the phone: \")\n",
    "\n",
    "if ip == \"0\":\n",
    "    cap = cv2.VideoCapture(0)\n",
    "else:\n",
    "    cap = cv2.VideoCapture(\"{}\".format(ip))\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 720)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Get frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    frame = cv2.resize(frame, (864, 704))\n",
    "    #print(frame.shape)\n",
    "    \n",
    "    # Object Detection\n",
    "    (class_ids, scores, bboxes) = model.detect(frame, confThreshold=0.1, nmsThreshold=.2)\n",
    "    for class_id, score, bbox in zip(class_ids, scores, bboxes):\n",
    "        (x, y, w, h) = bbox\n",
    "        class_name = classes[class_id]\n",
    "\n",
    "        cv2.putText(frame, class_name, (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 3, (50,200,25), 2)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (50,200,25), 3)\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "    key = cv2.waitKey(1) & 0xff\n",
    "    if key == ord(\"q\") or key == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb2e2e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you are not having the ip address, you can type: 0 \n",
      "Enter the ip address of the phone: 0\n",
      "Threshold:  0.2\n",
      "Threshold:  0.30000000000000004\n",
      "Threshold:  0.4\n",
      "Threshold:  0.5\n",
      "Threshold:  0.6\n",
      "Threshold:  0.7\n",
      "Threshold:  0.7999999999999999\n"
     ]
    }
   ],
   "source": [
    "# add the functionality adjustable threshold\n",
    "import cv2\n",
    "\n",
    "# Opencv DNN\n",
    "net = cv2.dnn.readNet(\"yolov4-tiny.weights\", \"yolov4-tiny.cfg\")\n",
    "model = cv2.dnn_DetectionModel(net)\n",
    "model.setInputParams(size=(416, 416), scale=1/255)\n",
    "\n",
    "# Load class lists\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as file_object:\n",
    "    for class_name in file_object.readlines():\n",
    "        class_name = class_name.strip()\n",
    "        classes.append(class_name)\n",
    "\n",
    "# Initialize camera\n",
    "print(\"If you are not having the ip address, you can type: 0 \")\n",
    "ip = input(\"Enter the ip address of the phone: \")\n",
    "if ip == \"0\":\n",
    "    cap = cv2.VideoCapture(0)\n",
    "else:\n",
    "    cap = cv2.VideoCapture(\"{}\".format(ip))\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 720)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Set default threshold\n",
    "threshold = 0.1\n",
    "\n",
    "while True:\n",
    "    # Get frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    frame = cv2.resize(frame, (864, 704))\n",
    "    \n",
    "    # Object Detection\n",
    "    (class_ids, scores, bboxes) = model.detect(frame, confThreshold=threshold, nmsThreshold=.2)\n",
    "    for class_id, score, bbox in zip(class_ids, scores, bboxes):\n",
    "        (x, y, w, h) = bbox\n",
    "        class_name = classes[class_id]\n",
    "\n",
    "        cv2.putText(frame, class_name, (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 3, (50,200,25), 2)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (50,200,25), 3)\n",
    "\n",
    "    # Display frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "    # Adjust threshold\n",
    "    key = cv2.waitKey(1) & 0xff\n",
    "    if key == ord(\"q\") or key == 27:\n",
    "        break\n",
    "    elif key == ord(\"+\"):\n",
    "        threshold += 0.1\n",
    "        print(\"Threshold: \", threshold)\n",
    "    elif key == ord(\"-\"):\n",
    "        threshold -= 0.1\n",
    "        if threshold < 0:\n",
    "            threshold = 0\n",
    "        print(\"Threshold: \", threshold)\n",
    "\n",
    "# Release capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80f8e757",
   "metadata": {},
   "source": [
    "http://192.168.29.61:8080/video"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40100f6e",
   "metadata": {},
   "source": [
    "# YOLOv5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ab2c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\DELL/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-2-26 Python-3.9.4 torch-2.2.1+cpu CPU\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m.pt to yolov5m.pt...\n",
      "100%|██████████| 40.8M/40.8M [00:38<00:00, 1.11MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients, 48.9 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you are not having the ip address, you can type: 0 \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5m', pretrained=True)\n",
    "\n",
    "# Load class names\n",
    "with open('coco.names', 'r', encoding='utf8') as f:\n",
    "    classes = [line.strip() for line in f.readlines()][0:]\n",
    "\n",
    "# Initialize camera\n",
    "print(\"If you are not having the ip address, you can type: 0 \")\n",
    "ip = input(\"Enter the ip address of the phone: \")\n",
    "if ip == \"0\":\n",
    "    cap = cv2.VideoCapture(0)\n",
    "else:\n",
    "    cap = cv2.VideoCapture(\"{}\".format(ip))\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 720)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Loop through frames\n",
    "while True:\n",
    "    # Read frame from camera\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Object detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # Loop through detected objects\n",
    "    for result in results.xyxy[0]:\n",
    "        class_id = int(result[5])\n",
    "        class_name = classes[class_id]\n",
    "        confidence = float(result[4])\n",
    "\n",
    "        # Draw bounding box around object\n",
    "        x1, y1, x2, y2 = [int(i) for i in result[:4]]\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (125, 150, 175), 2)\n",
    "\n",
    "        # Add label to bounding box\n",
    "        label = f\"{class_name} {confidence:.2f}\"\n",
    "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_PLAIN, 2, (125, 150, 175), 4)\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # Exit on 'q' key\n",
    "    key  = cv2.waitKey(1)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81452c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\DELL/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "WARNING  invalid check_version(5.9.5, ) requested, please check values.\n",
      "YOLOv5  2023-9-10 Python-3.10.7 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you don't have the IP address, enter: 0\n"
     ]
    }
   ],
   "source": [
    "#working condition\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Load class names\n",
    "with open('coco.names', 'r', encoding='utf8') as f:\n",
    "    classes = {i: line.strip() for i, line in enumerate(f)}\n",
    "\n",
    "# Initialize camera\n",
    "print(\"If you don't have the IP address, enter: 0\")\n",
    "ip_address = input(\"Enter the IP address of the phone: \")\n",
    "if ip_address == '0':\n",
    "    cap = cv2.VideoCapture(0)\n",
    "else:\n",
    "    cap = cv2.VideoCapture('http://' + ip_address + ':8080/video')\n",
    "# Check if camera is opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video stream from phone\")\n",
    "    exit()\n",
    "\n",
    "# Loop through frames\n",
    "while True:\n",
    "    # Read frame from camera\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading video stream from phone\")\n",
    "        break\n",
    "\n",
    "    # Object detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # Loop through detected objects\n",
    "    for result in results.xyxy[0]:\n",
    "        class_id = int(result[5])\n",
    "        class_name = classes[class_id]\n",
    "        confidence = float(result[4])\n",
    "\n",
    "        # Draw bounding box around object\n",
    "        x1, y1, x2, y2 = map(int, result[:4])\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (125, 150, 175), 2)\n",
    "\n",
    "        # Add label to bounding box\n",
    "        label = f\"{class_name} {confidence:.2f}\"\n",
    "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_PLAIN, 2, (125, 150, 175), 4)\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # Exit on 'q' key or ESC key\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "\n",
    "# Release camera and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28e83275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\DELL/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "WARNING  invalid check_version(5.9.5, ) requested, please check values.\n",
      "YOLOv5  2023-9-10 Python-3.10.7 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you don't have the IP address, enter: 0\n"
     ]
    }
   ],
   "source": [
    "#with threshold\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='E:/yolov5s.pt')\n",
    "\n",
    "# Load class names\n",
    "with open('coco.names', 'r', encoding='utf8') as f:\n",
    "    classes = {i: line.strip() for i, line in enumerate(f)}\n",
    "\n",
    "# Initialize camera\n",
    "print(\"If you don't have the IP address, enter: 0\")\n",
    "ip_address = input(\"Enter the IP address of the phone: \")\n",
    "if ip_address == '0':\n",
    "    cap = cv2.VideoCapture(0)\n",
    "else:\n",
    "    cap = cv2.VideoCapture('http://' + ip_address + ':8080/video')\n",
    "# Check if camera is opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video stream from phone\")\n",
    "    exit()\n",
    "\n",
    "# Confidence threshold for object detection\n",
    "conf_threshold = 0.3\n",
    "\n",
    "# Loop through frames\n",
    "while True:\n",
    "    # Read frame from camera\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading video stream from phone\")\n",
    "        break\n",
    "\n",
    "    # Object detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # Loop through detected objects\n",
    "    for result in results.xyxy[0]:\n",
    "        class_id = int(result[5])\n",
    "        class_name = classes[class_id]\n",
    "        confidence = float(result[4])\n",
    "\n",
    "        # Draw bounding box around object if confidence is above threshold\n",
    "        if confidence > conf_threshold:\n",
    "            x1, y1, x2, y2 = map(int, result[:4])\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (125, 150, 175), 2)\n",
    "\n",
    "            # Add label to bounding box\n",
    "            label = f\"{class_name} {confidence:.2f}\"\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_PLAIN, 2, (125, 150, 175), 4)\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # Exit on 'q' key or ESC key\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "\n",
    "    # Adjust confidence threshold up or down with arrow keys\n",
    "    if key == ord('+'):\n",
    "        conf_threshold += 0.1\n",
    "        print(f\"Confidence threshold increased to {conf_threshold:.1f}\")\n",
    "    elif key == ord('-'):\n",
    "        conf_threshold -= 0.1\n",
    "        if conf_threshold < 0:\n",
    "            conf_threshold = 0\n",
    "        print(f\"Confidence threshold decreased to {conf_threshold:.1f}\")\n",
    "\n",
    "# Release camera and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0152fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\DELL/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-4-10 Python-3.10.7 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you don't have the IP address, enter: 0\n",
      "Enter the IP address of the phone: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m frame \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(frame)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Object detection\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Move results to CPU\u001b[39;00m\n\u001b[0;32m     46\u001b[0m results\u001b[38;5;241m.\u001b[39mxyxy[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mxyxy[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\PYTHON\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\PYTHON\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:675\u001b[0m, in \u001b[0;36mAutoShape.forward\u001b[1;34m(self, ims, size, augment, profile)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ims, torch\u001b[38;5;241m.\u001b[39mTensor):  \u001b[38;5;66;03m# torch\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mautocast(autocast):\n\u001b[1;32m--> 675\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# inference\u001b[39;00m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;66;03m# Pre-process\u001b[39;00m\n\u001b[0;32m    678\u001b[0m n, ims \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(ims), \u001b[38;5;28mlist\u001b[39m(ims)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ims, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, [ims])  \u001b[38;5;66;03m# number, list of images\u001b[39;00m\n",
      "File \u001b[1;32mC:\\PYTHON\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:507\u001b[0m, in \u001b[0;36mDetectMultiBackend.forward\u001b[1;34m(self, im, augment, visualize)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, im, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, visualize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# YOLOv5 MultiBackend inference\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m     b, ch, h, w \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# batch, channel, height, width\u001b[39;00m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mand\u001b[39;00m im\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[0;32m    509\u001b[0m         im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mhalf()  \u001b[38;5;66;03m# to FP16\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available and select device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='E:/yolov5s.pt')\n",
    "model.to(device).eval()\n",
    "\n",
    "# Load class names\n",
    "with open('coco.names', 'r', encoding='utf8') as f:\n",
    "    classes = {i: line.strip() for i, line in enumerate(f)}\n",
    "\n",
    "# Initialize camera\n",
    "print(\"If you don't have the IP address, enter: 0\")\n",
    "ip_address = input(\"Enter the IP address of the phone: \")\n",
    "if ip_address == '0':\n",
    "    cap = cv2.VideoCapture(0)\n",
    "else:\n",
    "    cap = cv2.VideoCapture('http://' + ip_address + ':8080/video')\n",
    "# Check if camera is opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video stream from phone\")\n",
    "    exit()\n",
    "\n",
    "# Confidence threshold for object detection\n",
    "conf_threshold = 0.3\n",
    "\n",
    "# Loop through frames\n",
    "while True:\n",
    "    # Read frame from camera\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading video stream from phone\")\n",
    "        break\n",
    "\n",
    "    # Move frame to GPU device\n",
    "    frame = torch.from_numpy(frame).to(device)\n",
    "\n",
    "    # Object detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # Move results to CPU\n",
    "    results.xyxy[0] = results.xyxy[0].to('cpu')\n",
    "\n",
    "    # Loop through detected objects\n",
    "    for result in results.xyxy[0]:\n",
    "        class_id = int(result[5])\n",
    "        class_name = classes[class_id]\n",
    "        confidence = float(result[4])\n",
    "\n",
    "        # Draw bounding box around object if confidence is above threshold\n",
    "        if confidence > conf_threshold:\n",
    "            x1, y1, x2, y2 = map(int, result[:4])\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (125, 150, 175), 2)\n",
    "\n",
    "            # Add label to bounding box\n",
    "            label = f\"{class_name} {confidence:.2f}\"\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_PLAIN, 2, (125, 150, 175), 4)\n",
    "\n",
    "    # Move frame back to CPU for display\n",
    "    frame = frame.to('cpu').numpy()\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # Exit on 'q' key or ESC key\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q') or key == 27:\n",
    "        break\n",
    "\n",
    "    # Adjust confidence threshold up or down with arrow keys\n",
    "    if key == ord('+'):\n",
    "        conf_threshold += 0.1\n",
    "        print(f\"Confidence threshold increased to {conf_threshold:.1f}\")\n",
    "    elif key == ord('-'):\n",
    "        conf_threshold -= 0.1\n",
    "        if conf_threshold < 0:\n",
    "            conf_threshold = 0\n",
    "        print(f\"Confidence threshold decreased to {conf_threshold:.1f}\")\n",
    "\n",
    "# Release camera and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c33fde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
